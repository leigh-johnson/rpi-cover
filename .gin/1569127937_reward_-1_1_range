    root_dir,
    env_name='donkey-generated-track-multidiscrete-v0',
    num_iterations=100000,
    train_sequence_length=1,
    
    # Params for MultiDiscrete Gym Environment
    steer_actions=16, # subdivide range of steer controls (-1.0, 1.0) in n bins
    throttle_actions=3, # subdivide range of throttle controls into n bins

    # Params for QNetwork
    #fc_layer_params=[200, 100],
    # Params for QRnnNetwork
    input_fc_layer_params=(48,),
    lstm_size=(20,),
    output_fc_layer_params=(20,),

    # Params for collect
    boltzmann_temperature=None,
    #epsilon_greedy=0.1,
    epsilon_greedy=0.9,
    epsilon_decay=0.9,
    epsilon_decay_period=1,
    min_epsilon=0.1,
# Temperature value to use for Boltzmann sampling of
#         the actions during data collection. The closer to 0.0, the higher the
#         probability of choosing the best action.
    initial_collect_episodes=100,
    collect_episodes_per_iteration=1,
    replay_buffer_capacity=40000,
    # Params for target update
    target_update_tau=0.4,
    target_update_period=400,

    # Params for train
    train_steps_per_iteration=1,
    batch_size=256,
    learning_rate=1e-6,
    n_step_update=1,
    gamma=0.99,
    reward_scale_factor=0.99,
    gradient_clipping=None,
    use_tf_functions=True,
    # Params for eval
    num_eval_episodes=10,
    eval_interval=100,
    # Params for checkpoints
    train_checkpoint_interval=400,
    policy_checkpoint_interval=400,
    rb_checkpoint_interval=400,
    # Params for summaries and logging
    log_interval=1,
    summary_interval=1,
    summaries_flush_secs=10,
    debug_summaries=True,
    summarize_grads_and_vars=True,
        eval_metrics_callback=None,
     # donkey gym env
    sim=DONKEY_SIM_PATH,
    max_episode_steps=100000